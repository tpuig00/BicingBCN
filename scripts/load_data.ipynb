{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "def cargar_datos():\n",
    "    data_path = '../data'\n",
    "    files = os.listdir(data_path)\n",
    "    \n",
    "    df_list = []\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):  \n",
    "            file_path = os.path.join(data_path, file)\n",
    "\n",
    "            parts = file.split('_')\n",
    "\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            \n",
    "            year = parts[0]  \n",
    "            month_name = parts[2]  \n",
    "\n",
    "            if year not in ['2024']:\n",
    "                continue\n",
    "\n",
    "            if month_name not in ['Gener']:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_csv(file_path, low_memory=True, dtype=str)\n",
    "\n",
    "                df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
    "\n",
    "                df['year'] = df['last_reported'].dt.year\n",
    "                df['month'] = df['last_reported'].dt.month\n",
    "                df['day'] = df['last_reported'].dt.day\n",
    "                df['hour'] = df['last_reported'].dt.hour\n",
    "\n",
    "                # Eliminar columnas innecesarias si existen\n",
    "                df.drop(columns=[col for col in ['traffic', 'V1'] if col in df.columns], inplace=True)\n",
    "\n",
    "                # Convertir a float64 las columnas numéricas\n",
    "                numeric_cols = ['num_bikes_available', 'num_docks_available', \n",
    "                                'num_bikes_available_types.mechanical', 'num_bikes_available_types.ebike']\n",
    "                for col in numeric_cols:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = df[col].astype(float)\n",
    "\n",
    "                df_list.append(df)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar {file}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if df_list:\n",
    "        merged_df = pd.concat(df_list, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Cargar datos adicionales\n",
    "    df_2 = pd.read_csv('../Informacio_Estacions_Bicing_2025.csv', usecols=['station_id', 'lat', 'lon', 'capacity'], low_memory=False)\n",
    "\n",
    "    # Convertir station_id a Int64 para evitar errores de merge\n",
    "    merged_df['station_id'] = merged_df['station_id'].astype('Int64')\n",
    "    df_2['station_id'] = df_2['station_id'].astype('Int64')\n",
    "\n",
    "    # Pasar is_installed, is_renting, is_returning a int, manejando NaN\n",
    "    for col in ['is_installed', 'is_renting', 'is_returning']:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna(0).astype(int)\n",
    "\n",
    "    # Convertir 'TRUE'/'FALSE' de is_charging_station a 1/0\n",
    "    if 'is_charging_station' in merged_df.columns:\n",
    "        merged_df['is_charging_station'] = merged_df['is_charging_station'].map({'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "    # Merge con datos de estaciones\n",
    "    merged_df = merged_df.merge(df_2, on='station_id', how='inner')\n",
    "\n",
    "    # Calcular capacidad total y completar valores nulos\n",
    "    merged_df['sum_capacity'] = merged_df['num_bikes_available'] + merged_df['num_docks_available']\n",
    "    median_capacity = merged_df.groupby('station_id')['sum_capacity'].median()\n",
    "\n",
    "    merged_df['capacity'] = merged_df.apply(\n",
    "        lambda row: median_capacity[row['station_id']] if pd.isna(row['capacity']) else row['capacity'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Limitar num_docks_available a [0, capacity]\n",
    "    merged_df['num_docks_available'] = merged_df.apply(\n",
    "        lambda row: min(max(row['num_docks_available'], 0), row['capacity']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Calcular el target\n",
    "    merged_df['target'] = merged_df['num_docks_available'] / merged_df['capacity']\n",
    "    \n",
    "    # Agregación de datos\n",
    "    aggregated_df = merged_df.groupby(['station_id', 'year', 'month', 'day', 'hour']).agg(\n",
    "        num_bikes_available=('num_bikes_available', 'mean'),\n",
    "        num_docks_available=('num_docks_available', 'mean'),\n",
    "        num_mechanical=('num_bikes_available_types.mechanical', 'median'),\n",
    "        num_ebike=('num_bikes_available_types.ebike', 'median'),\n",
    "        # is_installed=('is_installed', 'mean'), No lo ponemos porque todos son true\n",
    "        is_renting=('is_renting', 'mean'),\n",
    "        is_returning=('is_returning', 'mean'),\n",
    "        target=('target', 'mean'),\n",
    "        lat=('lat', 'first'),\n",
    "        lon=('lon', 'first'),\n",
    "        capacity=('capacity', 'first')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Filtrar por estaciones disponibles en metadata\n",
    "    id = pd.read_csv('../data/metadata_sample_submission_2025.csv')\n",
    "    llista_stations = pd.unique(id['station_id'])\n",
    "    aggregated_df = aggregated_df[aggregated_df['station_id'].isin(llista_stations)]\n",
    "\n",
    "    return aggregated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AFEGIM ALTRES VARIABLES ( 4h anteriors + tipo de dia: festiu cap setmana laborable)\n",
    "\n",
    "def crear_campos_lags(df):\n",
    "   # df= df.iloc[4:] \n",
    "  \n",
    "    resultados = []\n",
    "    # Agrupamos por station_id\n",
    "    for station, grupo in df.groupby('station_id'):\n",
    "        # Aseguramos el orden cronológico\n",
    "        grupo = grupo.sort_values(by=['year', 'month', 'day', 'hour']).reset_index(drop=True)\n",
    "        n = len(grupo)\n",
    "        # Iteramos empezando en el índice 4 y avanzamos de 5 en 5\n",
    "        for i in range(4, n):\n",
    "            if i - 4 >= 0:\n",
    "                fila = grupo.loc[i].copy()\n",
    "                # Agregamos los valores de 'target' de las 4 horas previas\n",
    "                fila['ctx-4'] = grupo.loc[i - 4, 'target']\n",
    "                fila['ctx-3'] = grupo.loc[i - 3, 'target']\n",
    "                fila['ctx-2'] = grupo.loc[i - 2, 'target']\n",
    "                fila['ctx-1'] = grupo.loc[i - 1, 'target']\n",
    "                \n",
    "                resultados.append(fila)\n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "\n",
    "#funcio per afegir el tipo de dia, festiu laborable o cap setmana \n",
    "def day_categorization_bcn(df):\n",
    "    \"\"\"\n",
    "    Añade una columna 'day_type' al DataFrame con la clasificación numérica de cada día:\n",
    "    0 = Laborable, 1 = Fin de semana, 2 = Festivo.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "    # Festivos en Barcelona (2020 - marzo 2025)\n",
    "    holidays = [\n",
    "        \"2020-01-01\", \"2020-01-06\", \"2020-04-10\", \"2020-04-13\", \"2020-05-01\", \"2020-06-24\", \"2020-09-11\", \"2020-09-24\",\n",
    "        \"2020-10-12\", \"2020-11-01\", \"2020-12-06\", \"2020-12-08\", \"2020-12-25\", \"2020-12-26\",\n",
    "        \"2021-01-01\", \"2021-01-06\", \"2021-04-02\", \"2021-04-05\", \"2021-05-01\", \"2021-06-24\", \"2021-09-11\", \"2021-09-24\",\n",
    "        \"2021-10-12\", \"2021-11-01\", \"2021-12-06\", \"2021-12-08\", \"2021-12-25\", \"2021-12-26\",\n",
    "        \"2022-01-01\", \"2022-01-06\", \"2022-04-15\", \"2022-04-18\", \"2022-05-01\", \"2022-06-24\", \"2022-09-11\", \"2022-09-24\",\n",
    "        \"2022-10-12\", \"2022-11-01\", \"2022-12-06\", \"2022-12-08\", \"2022-12-25\", \"2022-12-26\",\n",
    "        \"2023-01-01\", \"2023-01-06\", \"2023-04-07\", \"2023-04-10\", \"2023-05-01\", \"2023-06-24\", \"2023-09-11\", \"2023-09-24\",\n",
    "        \"2023-10-12\", \"2023-11-01\", \"2023-12-06\", \"2023-12-08\", \"2023-12-25\", \"2023-12-26\",\n",
    "        \"2024-01-01\", \"2024-01-06\", \"2024-03-29\", \"2024-04-01\", \"2024-05-01\", \"2024-06-24\", \"2024-09-11\", \"2024-09-24\",\n",
    "        \"2024-10-12\", \"2024-11-01\", \"2024-12-06\", \"2024-12-08\", \"2024-12-25\", \"2024-12-26\",\n",
    "        \"2025-01-01\", \"2025-01-06\", \"2025-04-18\", \"2025-04-21\", \"2025-05-01\", \"2025-06-24\", \"2025-09-11\", \"2025-09-24\",\n",
    "        \"2025-10-12\", \"2025-11-01\", \"2025-12-06\", \"2025-12-08\", \"2025-12-25\", \"2025-12-26\"\n",
    "    ]\n",
    "    \n",
    "    holiday_dates = pd.to_datetime(holidays)\n",
    "\n",
    "    # Función para clasificar el día en valores numéricos\n",
    "    def classify_day(date):\n",
    "        if date in holiday_dates:\n",
    "            return 2  # Festivo\n",
    "        elif date.weekday() >= 5:  # Sábado (5) o domingo (6)\n",
    "            return 1  # Fin de semana\n",
    "        else:\n",
    "            return 0  # Laborable\n",
    "\n",
    "    df['day_type'] = df['date'].apply(classify_day)\n",
    "\n",
    "    df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/bj954x3n3xvfw__dvzkw3kb40000gn/T/ipykernel_11056/598774789.py:40: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "df_merge = cargar_datos()\n",
    "df_merge= crear_campos_lags(df_merge)\n",
    "df_merge_final=day_categorization_bcn(df_merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   station_id    year  month  day  hour  num_bikes_available  \\\n",
      "0         1.0  2024.0    1.0  1.0   2.0            14.500000   \n",
      "1         1.0  2024.0    1.0  1.0   7.0            13.000000   \n",
      "2         1.0  2024.0    1.0  1.0  12.0             4.666667   \n",
      "3         1.0  2024.0    1.0  1.0  17.0             5.666667   \n",
      "4         1.0  2024.0    1.0  1.0  22.0            10.000000   \n",
      "\n",
      "   num_docks_available  num_mechanical  num_ebike  is_renting  is_returning  \\\n",
      "0            30.500000             5.5        9.5         1.0           1.0   \n",
      "1            32.000000             9.0        4.0         1.0           1.0   \n",
      "2            40.333333             1.0        4.0         1.0           1.0   \n",
      "3            39.333333             2.0        4.0         1.0           1.0   \n",
      "4            35.000000             4.0        6.0         1.0           1.0   \n",
      "\n",
      "     target        lat       lon  capacity     ctx-4     ctx-3     ctx-2  \\\n",
      "0  0.663043  41.397978  2.180107      46.0  0.717391  0.768116  0.748188   \n",
      "1  0.695652  41.397978  2.180107      46.0  0.681159  0.733696  0.740803   \n",
      "2  0.876812  41.397978  2.180107      46.0  0.695652  0.715719  0.807971   \n",
      "3  0.855072  41.397978  2.180107      46.0  0.882246  0.864130  0.838768   \n",
      "4  0.760870  41.397978  2.180107      46.0  0.907609  0.898551  0.815217   \n",
      "\n",
      "      ctx-1  day_type  \n",
      "0  0.764493         2  \n",
      "1  0.721344         2  \n",
      "2  0.809783         2  \n",
      "3  0.807971         2  \n",
      "4  0.697464         2  \n"
     ]
    }
   ],
   "source": [
    "# Ahora con el df_merge_final crear un df nuevo que sea df_train que coja 1 de cada 5 filas\n",
    "df_train = df_merge_final.iloc[::5]\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "print(df_train.head())  # Ver las primeras filas del nuevo DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para separar los datos usamos df_train\n",
    "X = df_train[['station_id','month', 'day', 'hour', 'ctx-1', 'ctx-2', 'ctx-3', 'ctx-4','lat','lon', 'day_type']]\n",
    "y = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def linear_regression(X,y, size):\n",
    "#Separamos train y test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X ,y , test_size= size, random_state=42)\n",
    "\n",
    "    lm = LinearRegression()\n",
    "#Entrenamiento modelo\n",
    "    lm.fit(X_train, y_train)\n",
    "#Predict\n",
    "    y_pred = lm.predict(X_test)\n",
    "#Metricas\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    return  r2, mse, mae\n",
    "\n",
    "\n",
    "#RandomForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def RandomForest(X,y, size):\n",
    "#Separamos train y test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X ,y , test_size= size, random_state=42)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators = 100, random_state= 42)\n",
    "#Entrenamiento modelo\n",
    "    rf.fit(X_train, y_train)\n",
    "#Predict\n",
    "    y_pred = rf.predict(X_test)\n",
    "#Metricas\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "#Factoring Importance\n",
    "    feature_importance = rf.feature_importances_\n",
    "    feature_names = X_test.columns\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return  r2, mse, mae, importance_df, rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Métrica  Regresión Lineal  Random Forest\n",
      "0      R²          0.813425       0.821507\n",
      "1     MSE          0.011177       0.010693\n",
      "2     MAE          0.071522       0.070555\n",
      "Importancia de las características:\n",
      "       Feature  Importance\n",
      "4        ctx-1    0.826631\n",
      "5        ctx-2    0.038013\n",
      "6        ctx-3    0.022586\n",
      "7        ctx-4    0.021565\n",
      "0   station_id    0.019756\n",
      "9          lon    0.019517\n",
      "8          lat    0.017714\n",
      "3         hour    0.017167\n",
      "2          day    0.014763\n",
      "10    day_type    0.002288\n",
      "1        month    0.000000\n"
     ]
    }
   ],
   "source": [
    "r2_linear, mse_linear, mae_linear = linear_regression(X,y, 0.3)\n",
    "r2_rf, mse_rf, mae_rf, importance_df,rf = RandomForest(X, y, 0.3)\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Métrica': ['R²', 'MSE', 'MAE'],\n",
    "    'Regresión Lineal': [r2_linear, mse_linear, mae_linear],\n",
    "    'Random Forest': [r2_rf, mse_rf, mae_rf]\n",
    "})\n",
    "print(metrics_df)\n",
    "print(\"Importancia de las características:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('../metadata_sample_submission_2025.csv')\n",
    "df['year'] = 2025 #em faltava crear columna de year=2025 per poder aplicar funcio day_categorization_bcn \n",
    "\n",
    "\n",
    "# Función para imputar las columnas 'lat, lon' del df_merge_final en base (FK) la columa station_id\n",
    "# ---- IMPUTACIÓN DE COLUMNAS ADICIONALES ----\n",
    "cols_to_impute = ['lat', 'lon']\n",
    "\n",
    "# Usamos `merge` para traer la información de df_merge_final a df\n",
    "df = df.merge(df_merge_final[['station_id'] + cols_to_impute], on='station_id', how='left')\n",
    "df= day_categorization_bcn(df) \n",
    "\n",
    "# Seleccionar las características para predecir\n",
    "X_predict = df[['station_id', 'month', 'day', 'hour', 'ctx-4', 'ctx-3', 'ctx-2', 'ctx-1','lat','lon', 'day_type']]\n",
    "\n",
    "# Hacer predicciones\n",
    "predictions = rf.predict(X_predict)\n",
    "\n",
    "# Añadir las predicciones como una nueva columna en el DataFrame\n",
    "df['percentage_docks_available'] = predictions\n",
    "\n",
    "# Guardar el DataFrame con las predicciones en un nuevo archivo CSV\n",
    "df.to_csv('metadata_sample_submission_with_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predicciones añadidas y guardadas en 'metadata_sample_submission_with_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df[['index','percentage_docks_available']]\n",
    "df_final.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
