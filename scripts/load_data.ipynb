{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def cargar_datos():\n",
    "    data_path = '../data'\n",
    "    files = os.listdir(data_path)\n",
    "    \n",
    "    df_list = []\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):  \n",
    "            file_path = os.path.join(data_path, file)\n",
    "\n",
    "            parts = file.split('_')\n",
    "\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            \n",
    "            year = parts[0]  \n",
    "            month_name = parts[2]  \n",
    "\n",
    "            if year not in ['2024']:\n",
    "                continue\n",
    "\n",
    "            if month_name not in ['Gener']:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_csv(file_path, low_memory=True, dtype=str)\n",
    "\n",
    "                df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
    "\n",
    "                df['year'] = df['last_reported'].dt.year\n",
    "                df['month'] = df['last_reported'].dt.month\n",
    "                df['day'] = df['last_reported'].dt.day\n",
    "                df['hour'] = df['last_reported'].dt.hour\n",
    "\n",
    "                # Eliminar columnas innecesarias si existen\n",
    "                df.drop(columns=[col for col in ['traffic', 'V1'] if col in df.columns], inplace=True)\n",
    "\n",
    "                # Convertir a float64 las columnas numéricas\n",
    "                numeric_cols = ['num_bikes_available', 'num_docks_available', \n",
    "                                'num_bikes_available_types.mechanical', 'num_bikes_available_types.ebike']\n",
    "                for col in numeric_cols:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = df[col].astype(float)\n",
    "\n",
    "                df_list.append(df)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar {file}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if df_list:\n",
    "        merged_df = pd.concat(df_list, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Cargar datos adicionales\n",
    "    df_2 = pd.read_csv('../Informacio_Estacions_Bicing_2025.csv', usecols=['station_id', 'lat', 'lon', 'capacity'], low_memory=False)\n",
    "\n",
    "    # Convertir station_id a Int64 para evitar errores de merge\n",
    "    merged_df['station_id'] = merged_df['station_id'].astype('Int64')\n",
    "    df_2['station_id'] = df_2['station_id'].astype('Int64')\n",
    "\n",
    "    # Pasar is_installed, is_renting, is_returning a int, manejando NaN\n",
    "    for col in ['is_installed', 'is_renting', 'is_returning']:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna(0).astype(int)\n",
    "\n",
    "    # Convertir 'TRUE'/'FALSE' de is_charging_station a 1/0\n",
    "    if 'is_charging_station' in merged_df.columns:\n",
    "        merged_df['is_charging_station'] = merged_df['is_charging_station'].map({'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "    # Merge con datos de estaciones\n",
    "    merged_df = merged_df.merge(df_2, on='station_id', how='inner')\n",
    "\n",
    "    # Calcular capacidad total y completar valores nulos\n",
    "    merged_df['sum_capacity'] = merged_df['num_bikes_available'] + merged_df['num_docks_available']\n",
    "    median_capacity = merged_df.groupby('station_id')['sum_capacity'].median()\n",
    "\n",
    "    merged_df['capacity'] = merged_df.apply(\n",
    "        lambda row: median_capacity[row['station_id']] if pd.isna(row['capacity']) else row['capacity'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Limitar num_docks_available a [0, capacity]\n",
    "    merged_df['num_docks_available'] = merged_df.apply(\n",
    "        lambda row: min(max(row['num_docks_available'], 0), row['capacity']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Calcular el target\n",
    "    merged_df['target'] = merged_df['num_docks_available'] / merged_df['capacity']\n",
    "    \n",
    "    # Agregación de datos\n",
    "    aggregated_df = merged_df.groupby(['station_id', 'year', 'month', 'day', 'hour']).agg(\n",
    "        num_bikes_available=('num_bikes_available', 'mean'),\n",
    "        num_docks_available=('num_docks_available', 'mean'),\n",
    "        num_mechanical=('num_bikes_available_types.mechanical', 'median'),\n",
    "        num_ebike=('num_bikes_available_types.ebike', 'median'),\n",
    "        # is_installed=('is_installed', 'mean'), No lo ponemos porque todos son true\n",
    "        is_renting=('is_renting', 'mean'),\n",
    "        is_returning=('is_returning', 'mean'),\n",
    "        target=('target', 'mean'),\n",
    "        lat=('lat', 'first'),\n",
    "        lon=('lon', 'first'),\n",
    "        capacity=('capacity', 'first')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Filtrar por estaciones disponibles en metadata\n",
    "    id = pd.read_csv('../data/metadata_sample_submission_2025.csv')\n",
    "    llista_stations = pd.unique(id['station_id'])\n",
    "    aggregated_df = aggregated_df[aggregated_df['station_id'].isin(llista_stations)]\n",
    "\n",
    "    return aggregated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55398/3696208945.py:38: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "df_merge = cargar_datos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   station_id    year  month  day  hour  num_bikes_available  \\\n",
      "4         1.0  2024.0    1.0  1.0   6.0            11.818182   \n",
      "5         1.0  2024.0    1.0  1.0   7.0            13.000000   \n",
      "6         1.0  2024.0    1.0  1.0   8.0            13.000000   \n",
      "7         1.0  2024.0    1.0  1.0   9.0            12.076923   \n",
      "8         1.0  2024.0    1.0  1.0  10.0             7.833333   \n",
      "\n",
      "   num_docks_available  num_mechanical  num_ebike  is_renting  is_returning  \\\n",
      "4            33.181818             9.0        3.0         1.0           1.0   \n",
      "5            32.000000             9.0        4.0         1.0           1.0   \n",
      "6            32.000000             9.0        4.0         1.0           1.0   \n",
      "7            32.923077             9.0        3.0         1.0           1.0   \n",
      "8            37.166667             4.0        3.0         1.0           1.0   \n",
      "\n",
      "     target        lat       lon  capacity     ctx-4     ctx-3     ctx-2  \\\n",
      "4  0.721344  41.397978  2.180107      46.0  0.663043  0.681159  0.733696   \n",
      "5  0.695652  41.397978  2.180107      46.0  0.681159  0.733696  0.740803   \n",
      "6  0.695652  41.397978  2.180107      46.0  0.733696  0.740803  0.721344   \n",
      "7  0.715719  41.397978  2.180107      46.0  0.740803  0.721344  0.695652   \n",
      "8  0.807971  41.397978  2.180107      46.0  0.721344  0.695652  0.695652   \n",
      "\n",
      "      ctx-1  \n",
      "4  0.740803  \n",
      "5  0.721344  \n",
      "6  0.695652  \n",
      "7  0.695652  \n",
      "8  0.715719  \n"
     ]
    }
   ],
   "source": [
    "#suposant que comencem per l'inici dun dia(elimino les 4 hores del dia anterior del mes anterior, ja que per defecte surten)\n",
    "df_merge = df_merge.iloc[4:]\n",
    "df_merge.head()  \n",
    "\n",
    "#funcio per afegir les 4h anteriors\n",
    "def crear_campos_lags(df):\n",
    "    resultados = []\n",
    "    # Agrupamos por station_id\n",
    "    for station, grupo in df.groupby('station_id'):\n",
    "        # Aseguramos el orden cronológico\n",
    "        grupo = grupo.sort_values(by=['year', 'month', 'day', 'hour']).reset_index(drop=True)\n",
    "        n = len(grupo)\n",
    "        # Iteramos empezando en el índice 4 y avanzamos de 5 en 5\n",
    "        for i in range(4, n):\n",
    "            if i - 4 >= 0:\n",
    "                fila = grupo.loc[i].copy()\n",
    "                # Agregamos los valores de 'target' de las 4 horas previas\n",
    "                fila['ctx-4'] = grupo.loc[i - 4, 'target']\n",
    "                fila['ctx-3'] = grupo.loc[i - 3, 'target']\n",
    "                fila['ctx-2'] = grupo.loc[i - 2, 'target']\n",
    "                fila['ctx-1'] = grupo.loc[i - 1, 'target']\n",
    "                \n",
    "                resultados.append(fila)\n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "# Aplicamos la función al DataFrame df_merge\n",
    "df_merge_final = crear_campos_lags(df_merge)\n",
    "\n",
    "print(df_merge_final.head())  # Ver las primeras filas del nuevo DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   station_id    year  month  day  hour  num_bikes_available  \\\n",
      "0         1.0  2024.0    1.0  1.0   6.0            11.818182   \n",
      "1         1.0  2024.0    1.0  1.0  11.0             7.666667   \n",
      "2         1.0  2024.0    1.0  1.0  16.0             7.750000   \n",
      "3         1.0  2024.0    1.0  1.0  21.0            12.916667   \n",
      "4         1.0  2024.0    1.0  2.0   2.0             9.000000   \n",
      "\n",
      "   num_docks_available  num_mechanical  num_ebike  is_renting  is_returning  \\\n",
      "0            33.181818             9.0        3.0         1.0           1.0   \n",
      "1            37.250000             3.0        5.0         1.0           1.0   \n",
      "2            37.166667             0.0        7.5         1.0           1.0   \n",
      "3            32.083333             3.0        9.5         1.0           1.0   \n",
      "4            36.000000             4.0        5.0         1.0           1.0   \n",
      "\n",
      "     target        lat       lon  capacity     ctx-4     ctx-3     ctx-2  \\\n",
      "0  0.721344  41.397978  2.180107      46.0  0.663043  0.681159  0.733696   \n",
      "1  0.809783  41.397978  2.180107      46.0  0.695652  0.695652  0.715719   \n",
      "2  0.807971  41.397978  2.180107      46.0  0.876812  0.882246  0.864130   \n",
      "3  0.697464  41.397978  2.180107      46.0  0.855072  0.907609  0.898551   \n",
      "4  0.782609  41.397978  2.180107      46.0  0.760870  0.797101  0.773551   \n",
      "\n",
      "      ctx-1  \n",
      "0  0.740803  \n",
      "1  0.807971  \n",
      "2  0.838768  \n",
      "3  0.815217  \n",
      "4  0.782609  \n"
     ]
    }
   ],
   "source": [
    "# Ahora con el df_merge_final crear un df nuevo que sea df_train que coja 1 de cada 5 filas\n",
    "df_train = df_merge_final.iloc[::5]\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "print(df_train.head())  # Ver las primeras filas del nuevo DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para separar los datos usamos df_train\n",
    "X = df_train[['station_id','month', 'day', 'hour', 'ctx-1', 'ctx-2', 'ctx-3', 'ctx-4','lat','lon']]\n",
    "y = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def linear_regression(X,y, size):\n",
    "#Separamos train y test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X ,y , test_size= size, random_state=42)\n",
    "\n",
    "    lm = LinearRegression()\n",
    "#Entrenamiento modelo\n",
    "    lm.fit(X_train, y_train)\n",
    "#Predict\n",
    "    y_pred = lm.predict(X_test)\n",
    "#Metricas\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    return  r2, mse, mae\n",
    "\n",
    "\n",
    "#RandomForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def RandomForest(X,y, size):\n",
    "#Separamos train y test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X ,y , test_size= size, random_state=42)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators = 100, random_state= 42)\n",
    "#Entrenamiento modelo\n",
    "    rf.fit(X_train, y_train)\n",
    "#Predict\n",
    "    y_pred = rf.predict(X_test)\n",
    "#Metricas\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "#Factoring Importance\n",
    "    feature_importance = rf.feature_importances_\n",
    "    feature_names = X_test.columns\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return  r2, mse, mae, importance_df, rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Métrica  Regresión Lineal  Random Forest\n",
      "0      R²          0.807260       0.812416\n",
      "1     MSE          0.011483       0.011176\n",
      "2     MAE          0.072257       0.071597\n",
      "Importancia de las características:\n",
      "      Feature  Importance\n",
      "4       ctx-1    0.824966\n",
      "5       ctx-2    0.038820\n",
      "6       ctx-3    0.022982\n",
      "7       ctx-4    0.021907\n",
      "0  station_id    0.020130\n",
      "9         lon    0.019853\n",
      "8         lat    0.018047\n",
      "3        hour    0.017378\n",
      "2         day    0.015916\n",
      "1       month    0.000000\n"
     ]
    }
   ],
   "source": [
    "r2_linear, mse_linear, mae_linear = linear_regression(X,y, 0.3)\n",
    "r2_rf, mse_rf, mae_rf, importance_df,rf = RandomForest(X, y, 0.3)\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Métrica': ['R²', 'MSE', 'MAE'],\n",
    "    'Regresión Lineal': [r2_linear, mse_linear, mae_linear],\n",
    "    'Random Forest': [r2_rf, mse_rf, mae_rf]\n",
    "})\n",
    "print(metrics_df)\n",
    "print(\"Importancia de las características:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('../metadata_sample_submission_2025.csv')\n",
    "\n",
    "\n",
    "# Función para imputar las columnas 'lat, lon' del df_merge_final en base (FK) la columa station_id\n",
    "# ---- IMPUTACIÓN DE COLUMNAS ADICIONALES ----\n",
    "cols_to_impute = ['lat', 'lon']\n",
    "\n",
    "# Usamos `merge` para traer la información de df_merge_final a df\n",
    "df = df.merge(df_merge_final[['station_id'] + cols_to_impute], on='station_id', how='left')\n",
    "\n",
    "# Seleccionar las características para predecir\n",
    "X_predict = df[['station_id', 'month', 'day', 'hour', 'ctx-4', 'ctx-3', 'ctx-2', 'ctx-1','lat','lon']]\n",
    "\n",
    "# Hacer predicciones\n",
    "predictions = rf.predict(X_predict)\n",
    "\n",
    "# Añadir las predicciones como una nueva columna en el DataFrame\n",
    "df['predictions'] = predictions\n",
    "\n",
    "# Guardar el DataFrame con las predicciones en un nuevo archivo CSV\n",
    "df.to_csv('metadata_sample_submission_with_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predicciones añadidas y guardadas en 'metadata_sample_submission_with_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df[['index','predictions']]\n",
    "df_final.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
