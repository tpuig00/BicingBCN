{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "from dask.distributed import Client\n",
    "\n",
    "def inicialize_dask():\n",
    "    # Inicializa un cliente de Dask y ponle puerto 8786\n",
    "    client = Client(memory_limit='8GB', processes=False)\n",
    "    print(client)\n",
    "\n",
    "def cargar_datos():\n",
    "\n",
    "    inicialize_dask()\n",
    "\n",
    "    data_path = '../data'\n",
    "    files = os.listdir(data_path)\n",
    "    \n",
    "    # Filtra y recopila únicamente los archivos que cumplen con la convención de nombres\n",
    "    selected_files = []\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            parts = file.split('_')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            year = parts[0]\n",
    "            if year not in ['2024', '2023','2022']:\n",
    "                continue\n",
    "            month = parts[1]\n",
    "            if year == '2023' and month not in ['01','03','06','09','11']:\n",
    "                continue\n",
    "            if year == '2022' and month not in ['01','03','06','09','11']:\n",
    "                continue\n",
    "            selected_files.append(os.path.join(data_path, file))\n",
    "    \n",
    "    # Función para procesar cada archivo, decorada con @delayed para que se ejecute de forma perezosa\n",
    "    @delayed\n",
    "    def process_file(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, low_memory=True, dtype=str, \n",
    "             usecols=['station_id', 'last_reported', 'num_bikes_available',\n",
    "             'num_docks_available', 'num_bikes_available_types.mechanical', 'num_bikes_available_types.ebike', \n",
    "             'is_installed', 'is_renting', 'is_returning', 'is_charging_station'],\n",
    "            skiprows=lambda i: i > 0 and i % 3 != 0)\n",
    "            df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
    "            df['year'] = df['last_reported'].dt.year\n",
    "            df['month'] = df['last_reported'].dt.month\n",
    "            df['day'] = df['last_reported'].dt.day\n",
    "            df['hour'] = df['last_reported'].dt.hour\n",
    "\n",
    "            # Elimina columnas innecesarias si existen\n",
    "            cols_to_drop = [col for col in ['traffic', 'V1'] if col in df.columns]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "            # Convierte a float las columnas numéricas\n",
    "            numeric_cols = ['num_bikes_available', 'num_docks_available', \n",
    "                            'num_bikes_available_types.mechanical', 'num_bikes_available_types.ebike']\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype(float)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()  # Devuelve un DataFrame vacío en caso de error\n",
    "\n",
    "    # Crea una lista de DataFrames diferidos (lazy)\n",
    "    delayed_dfs = [process_file(file_path) for file_path in selected_files]\n",
    "    \n",
    "    if not delayed_dfs:\n",
    "        return None\n",
    "\n",
    "    # Convierte los objetos diferidos en un Dask DataFrame\n",
    "    ddf = dd.from_delayed(delayed_dfs)\n",
    "\n",
    "    # Carga la metadata de estaciones en un DataFrame de pandas (pequeño)\n",
    "    df_meta = pd.read_csv('../Informacio_Estacions_Bicing_2025.csv',\n",
    "                          usecols=['station_id', 'lat', 'lon', 'capacity'],\n",
    "                          low_memory=False)\n",
    "    # Asegúrate de que los tipos de 'station_id' coincidan\n",
    "    ddf['station_id'] = ddf['station_id'].astype('Int64')\n",
    "    df_meta['station_id'] = df_meta['station_id'].astype('Int64')\n",
    "\n",
    "    # Procesa columnas que necesitan conversiones de tipo\n",
    "    for col in ['is_installed', 'is_renting', 'is_returning']:\n",
    "        if col in ddf.columns:\n",
    "            ddf[col] = ddf[col].fillna(0).astype(int)\n",
    "    \n",
    "    if 'is_charging_station' in ddf.columns:\n",
    "        ddf['is_charging_station'] = ddf['is_charging_station'].map({'TRUE': 1, 'FALSE': 0})\n",
    "    \n",
    "    # Realiza el merge con la metadata de estaciones (la metadata se carga en memoria)\n",
    "    ddf = ddf.merge(df_meta, on='station_id', how='inner')\n",
    "    \n",
    "    # Calcula una columna adicional de capacidad total\n",
    "    ddf['sum_capacity'] = ddf['num_bikes_available'] + ddf['num_docks_available']\n",
    "    \n",
    "    # Convierte a pandas para operaciones que se realizan de forma vectorizada\n",
    "    df_final = ddf.compute()\n",
    "    \n",
    "    # Reemplaza los valores nulos en 'capacity' utilizando la mediana por estación (operación vectorizada)\n",
    "    median_capacity = df_final.groupby('station_id')['sum_capacity'].median()\n",
    "    df_final['capacity'] = df_final['capacity'].fillna(df_final['station_id'].map(median_capacity))\n",
    "    \n",
    "    # Limita 'num_docks_available' entre 0 y 'capacity' usando la función clip, que es vectorizada\n",
    "    df_final['num_docks_available'] = df_final['num_docks_available'].clip(lower=0, upper=df_final['capacity'])\n",
    "    \n",
    "    # Calcula la variable objetivo\n",
    "    df_final['target'] = df_final['num_docks_available'] / df_final['capacity']\n",
    "    \n",
    "    # Agrega los datos por estación y componentes temporales\n",
    "    aggregated_df = df_final.groupby(['station_id', 'year', 'month', 'day', 'hour']).agg(\n",
    "        num_bikes_available=('num_bikes_available', 'mean'),\n",
    "        num_docks_available=('num_docks_available', 'mean'),\n",
    "        num_mechanical=('num_bikes_available_types.mechanical', 'median'),\n",
    "        num_ebike=('num_bikes_available_types.ebike', 'median'),\n",
    "        is_renting=('is_renting', 'mean'),\n",
    "        is_returning=('is_returning', 'mean'),\n",
    "        target=('target', 'mean'),\n",
    "        lat=('lat', 'first'),\n",
    "        lon=('lon', 'first'),\n",
    "        capacity=('capacity', 'first')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Filtra por las estaciones disponibles en la metadata de envío\n",
    "    id_df = pd.read_csv('../data/metadata_sample_submission_2025.csv')\n",
    "    station_list = pd.unique(id_df['station_id'])\n",
    "    aggregated_df = aggregated_df[aggregated_df['station_id'].isin(station_list)]\n",
    "    \n",
    "    return aggregated_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "from dask.distributed import Client\n",
    "\n",
    "def inicialize_dask():\n",
    "    # Inicializa un cliente de Dask con límite de memoria y sin procesos (para reducir sobrecarga)\n",
    "    client = Client(memory_limit='8GB', processes=False)\n",
    "    print(client)\n",
    "\n",
    "def cargar_datos():\n",
    "    inicialize_dask()\n",
    "\n",
    "    data_path = '../data'\n",
    "    files = os.listdir(data_path)\n",
    "    \n",
    "    # Filtra y recopila únicamente los archivos que cumplen con la convención de nombres\n",
    "    selected_files = []\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            parts = file.split('_')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            year = parts[0]\n",
    "            if year not in ['2024', '2023']:\n",
    "                continue\n",
    "            selected_files.append(os.path.join(data_path, file))\n",
    "    \n",
    "    # Función para procesar cada archivo, ejecutada de forma perezosa\n",
    "    @delayed\n",
    "    def process_file(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, low_memory=True, dtype=str)\n",
    "            df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
    "            df['year'] = df['last_reported'].dt.year\n",
    "            df['month'] = df['last_reported'].dt.month\n",
    "            df['day'] = df['last_reported'].dt.day\n",
    "            df['hour'] = df['last_reported'].dt.hour\n",
    "\n",
    "            # Elimina columnas innecesarias\n",
    "            cols_to_drop = [col for col in ['traffic', 'V1'] if col in df.columns]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "            # Convierte a numérico las columnas relevantes\n",
    "            numeric_cols = ['num_bikes_available', 'num_docks_available', \n",
    "                            'num_bikes_available_types.mechanical', 'num_bikes_available_types.ebike']\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()  # Devuelve un DataFrame vacío en caso de error\n",
    "\n",
    "    delayed_dfs = [process_file(file_path) for file_path in selected_files]\n",
    "    if not delayed_dfs:\n",
    "        return None\n",
    "\n",
    "    # Crea un Dask DataFrame a partir de los DataFrames diferidos\n",
    "    ddf = dd.from_delayed(delayed_dfs)\n",
    "\n",
    "    # Carga la metadata de estaciones\n",
    "    df_meta = pd.read_csv('../Informacio_Estacions_Bicing_2025.csv',\n",
    "                          usecols=['station_id', 'lat', 'lon', 'capacity'],\n",
    "                          low_memory=True)\n",
    "    \n",
    "    ddf['station_id'] = ddf['station_id'].astype('Int64')\n",
    "    df_meta['station_id'] = df_meta['station_id'].astype('Int64')\n",
    "\n",
    "    # Procesa columnas adicionales si existen\n",
    "    for col in ['is_installed', 'is_renting', 'is_returning']:\n",
    "        if col in ddf.columns:\n",
    "            ddf[col] = ddf[col].fillna(0).astype(int)\n",
    "    if 'is_charging_station' in ddf.columns:\n",
    "        ddf['is_charging_station'] = ddf['is_charging_station'].map({'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "    # Realiza el merge con la metadata de estaciones\n",
    "    ddf = ddf.merge(df_meta, on='station_id', how='inner')\n",
    "\n",
    "    # Calcula una columna adicional: capacidad total según datos disponibles\n",
    "    ddf['sum_capacity'] = ddf['num_bikes_available'] + ddf['num_docks_available']\n",
    "\n",
    "    # Calcula la mediana de 'sum_capacity' por estación usando Pandas en un subconjunto (solo dos columnas)\n",
    "    median_capacity = ddf[['station_id', 'sum_capacity']].compute().groupby('station_id')['sum_capacity'].median()\n",
    "\n",
    "    # Función para rellenar valores nulos en 'capacity' usando la mediana por estación\n",
    "    def fill_capacity(df, median_capacity):\n",
    "        df['capacity'] = df['capacity'].fillna(df['station_id'].map(median_capacity))\n",
    "        return df\n",
    "\n",
    "    # Aplica la función en cada partición sin materializar todo el dataset\n",
    "    ddf = ddf.map_partitions(fill_capacity, median_capacity)\n",
    "\n",
    "    # Limita 'num_docks_available' entre 0 y 'capacity'\n",
    "    ddf['num_docks_available'] = ddf['num_docks_available'].clip(lower=0, upper=ddf['capacity'])\n",
    "    \n",
    "    # Calcula la variable objetivo\n",
    "    ddf['target'] = ddf['num_docks_available'] / ddf['capacity']\n",
    "\n",
    "    # Agrega los datos por estación y componentes temporales utilizando operaciones distribuidas\n",
    "    aggregated_ddf = ddf.groupby(['station_id', 'year', 'month', 'day', 'hour']).agg({\n",
    "        'num_bikes_available': 'mean',\n",
    "        'num_docks_available': 'mean',\n",
    "        'num_bikes_available_types.mechanical': 'median',\n",
    "        'num_bikes_available_types.ebike': 'median',\n",
    "        'is_renting': 'mean',\n",
    "        'is_returning': 'mean',\n",
    "        'target': 'mean',\n",
    "        'lat': 'first',\n",
    "        'lon': 'first',\n",
    "        'capacity': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Filtra por las estaciones disponibles en la metadata de envío\n",
    "    id_df = pd.read_csv('../data/metadata_sample_submission_2025.csv')\n",
    "    station_list = pd.unique(id_df['station_id'])\n",
    "    aggregated_ddf = aggregated_ddf[aggregated_ddf['station_id'].isin(station_list)]\n",
    "    \n",
    "    # Finalmente, computa el resultado agregado (mucho menos datos que materializar todo el DataFrame)\n",
    "    aggregated_df = aggregated_ddf.compute()\n",
    "    \n",
    "    return aggregated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AFEGIM ALTRES VARIABLES ( 4h anteriors + tipo de dia: festiu cap setmana laborable)\n",
    "\n",
    "def crear_campos_optimized(df):\n",
    "    df = df.sort_values(by=['station_id', 'year', 'month', 'day', 'hour']).reset_index(drop=True)\n",
    "    \n",
    "    # Aplicamos los lags por grupo\n",
    "    def aplicar_lags(grupo):\n",
    "        for lag in range(1, 5):\n",
    "            grupo[f'ctx-{lag}'] = grupo['target'].shift(lag)\n",
    "        return grupo\n",
    "\n",
    "    df = df.groupby('station_id').apply(aplicar_lags).reset_index(drop=True)\n",
    "\n",
    "    # Eliminamos las primeras 4 filas de cada grupo y muestreamos cada 5 horas\n",
    "    def filtro(grupo):\n",
    "        grupo = grupo.iloc[4:]  # quitamos las primeras 4\n",
    "        return grupo.iloc[::5]  # cogemos 1 de cada 5\n",
    "\n",
    "    df = df.groupby('station_id').apply(filtro).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#funcio per afegir el tipo de dia, festiu laborable o cap setmana \n",
    "def day_categorization_bcn(df):\n",
    "    \"\"\"\n",
    "    Añade una columna 'day_type' al DataFrame con la clasificación numérica de cada día:\n",
    "    0 = Laborable, 1 = Fin de semana, 2 = Festivo.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "    # Festivos en Barcelona (2020 - marzo 2025)\n",
    "    holidays = [\n",
    "        \"2020-01-01\", \"2020-01-06\", \"2020-04-10\", \"2020-04-13\", \"2020-05-01\", \"2020-06-24\", \"2020-09-11\", \"2020-09-24\",\n",
    "        \"2020-10-12\", \"2020-11-01\", \"2020-12-06\", \"2020-12-08\", \"2020-12-25\", \"2020-12-26\",\n",
    "        \"2021-01-01\", \"2021-01-06\", \"2021-04-02\", \"2021-04-05\", \"2021-05-01\", \"2021-06-24\", \"2021-09-11\", \"2021-09-24\",\n",
    "        \"2021-10-12\", \"2021-11-01\", \"2021-12-06\", \"2021-12-08\", \"2021-12-25\", \"2021-12-26\",\n",
    "        \"2022-01-01\", \"2022-01-06\", \"2022-04-15\", \"2022-04-18\", \"2022-05-01\", \"2022-06-24\", \"2022-09-11\", \"2022-09-24\",\n",
    "        \"2022-10-12\", \"2022-11-01\", \"2022-12-06\", \"2022-12-08\", \"2022-12-25\", \"2022-12-26\",\n",
    "        \"2023-01-01\", \"2023-01-06\", \"2023-04-07\", \"2023-04-10\", \"2023-05-01\", \"2023-06-24\", \"2023-09-11\", \"2023-09-24\",\n",
    "        \"2023-10-12\", \"2023-11-01\", \"2023-12-06\", \"2023-12-08\", \"2023-12-25\", \"2023-12-26\",\n",
    "        \"2024-01-01\", \"2024-01-06\", \"2024-03-29\", \"2024-04-01\", \"2024-05-01\", \"2024-06-24\", \"2024-09-11\", \"2024-09-24\",\n",
    "        \"2024-10-12\", \"2024-11-01\", \"2024-12-06\", \"2024-12-08\", \"2024-12-25\", \"2024-12-26\",\n",
    "        \"2025-01-01\", \"2025-01-06\", \"2025-04-18\", \"2025-04-21\", \"2025-05-01\", \"2025-06-24\", \"2025-09-11\", \"2025-09-24\",\n",
    "        \"2025-10-12\", \"2025-11-01\", \"2025-12-06\", \"2025-12-08\", \"2025-12-25\", \"2025-12-26\"\n",
    "    ]\n",
    "    \n",
    "    holiday_dates = pd.to_datetime(holidays)\n",
    "\n",
    "    # Función para clasificar el día en valores numéricos\n",
    "    def classify_day(date):\n",
    "        if date in holiday_dates:\n",
    "            return 2  # Festivo\n",
    "        elif date.weekday() >= 5:  # Sábado (5) o domingo (6)\n",
    "            return 1  # Fin de semana\n",
    "        else:\n",
    "            return 0  # Laborable\n",
    "\n",
    "    df['day_type'] = df['date'].apply(classify_day)\n",
    "\n",
    "    df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'inproc://172.20.10.6/17524/1' processes=1 threads=12, memory=7.45 GiB>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/home/tplan/anaconda3/lib/python3.12/site-packages/dask_expr/_collection.py:4192: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('is_charging_station', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "/tmp/ipykernel_17524/3434399164.py:43: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "df_merge = cargar_datos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17524/107475075.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('station_id').apply(aplicar_lags).reset_index(drop=True)\n",
      "/tmp/ipykernel_17524/107475075.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('station_id').apply(filtro).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "df_merge= crear_campos_optimized(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_final=day_categorization_bcn(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   station_id    year  month  day  hour  num_bikes_available  \\\n",
      "0           1  2023.0    1.0  1.0  23.0            18.666667   \n",
      "1           1  2023.0    1.0  2.0   4.0            22.000000   \n",
      "2           1  2023.0    1.0  2.0  12.0            13.000000   \n",
      "3           1  2023.0    1.0  2.0  20.0            10.500000   \n",
      "4           1  2023.0    1.0  3.0   1.0            10.000000   \n",
      "5           1  2023.0    1.0  3.0   6.0            18.500000   \n",
      "6           1  2023.0    1.0  3.0  11.0             1.500000   \n",
      "7           1  2023.0    1.0  3.0  16.0             5.500000   \n",
      "8           1  2023.0    1.0  3.0  21.0            10.500000   \n",
      "9           1  2023.0    1.0  4.0   2.0            12.000000   \n",
      "\n",
      "   num_docks_available  num_mechanical  num_ebike  is_renting  is_returning  \\\n",
      "0            27.333333            18.0        0.0         1.0           1.0   \n",
      "1            24.000000            22.0        0.0         1.0           1.0   \n",
      "2            33.000000            12.0        1.0         1.0           1.0   \n",
      "3            35.500000            10.0        0.5         1.0           1.0   \n",
      "4            36.000000            10.0        0.0         1.0           1.0   \n",
      "5            27.500000            18.5        0.0         1.0           1.0   \n",
      "6            44.500000             1.5        0.0         1.0           1.0   \n",
      "7            40.500000             5.5        0.0         1.0           1.0   \n",
      "8            35.000000             9.5        1.0         1.0           1.0   \n",
      "9            33.000000            11.0        1.0         1.0           1.0   \n",
      "\n",
      "     target        lat       lon  capacity     ctx-1     ctx-2     ctx-3  \\\n",
      "0  0.594203  41.397978  2.180107        46  0.597826  0.543478  0.333333   \n",
      "1  0.521739  41.397978  2.180107        46  0.521739  0.521739  0.507246   \n",
      "2  0.717391  41.397978  2.180107        46  0.695652  0.630435  0.557971   \n",
      "3  0.771739  41.397978  2.180107        46  0.717391  0.739130  0.760870   \n",
      "4  0.782609  41.397978  2.180107        46  0.789855  0.804348  0.768116   \n",
      "5  0.597826  41.397978  2.180107        46  0.536232  0.521739  0.789855   \n",
      "6  0.967391  41.397978  2.180107        46  0.905797  0.804348  0.746377   \n",
      "7  0.880435  41.397978  2.180107        46  0.797101  0.728261  0.782609   \n",
      "8  0.760870  41.397978  2.180107        46  0.811594  0.913043  0.942029   \n",
      "9  0.717391  41.397978  2.180107        46  0.731884  0.750000  0.775362   \n",
      "\n",
      "      ctx-4  day_type  \n",
      "0  0.228261         2  \n",
      "1  0.489130         0  \n",
      "2  0.521739         0  \n",
      "3  0.750000         0  \n",
      "4  0.706522         0  \n",
      "5  0.782609         0  \n",
      "6  0.619565         0  \n",
      "7  0.902174         0  \n",
      "8  0.858696         0  \n",
      "9  0.750000         0  \n"
     ]
    }
   ],
   "source": [
    "# Ahora con el df_merge_final crear un df nuevo que sea df_train que coja 1 de cada 5 filas\n",
    "# df_train = df_merge_final.iloc[::5]\n",
    "# df_train = df_train.reset_index(drop=True)\n",
    "print(df_merge_final.head(10))  # Ver las primeras filas del nuevo DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para separar los datos usamos df_train\n",
    "X = df_merge_final[['station_id','month', 'day', 'hour', 'ctx-1', 'ctx-2', 'ctx-3', 'ctx-4','lat','lon', 'day_type']]\n",
    "y = df_merge_final['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "def linear_regression(X,y, size):\n",
    "#Separamos train y test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X ,y , test_size= size, random_state=42)\n",
    "\n",
    "    lm = LinearRegression()\n",
    "#Entrenamiento modelo\n",
    "    lm.fit(X_train, y_train)\n",
    "#Predict\n",
    "    y_pred = lm.predict(X_test)\n",
    "#Metricas\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    return  r2, mse, mae\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "def RandomForest(X,y, size):\n",
    "#Separamos train y test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X ,y , test_size= size, random_state=42)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators = 100, random_state= 42)\n",
    "#Entrenamiento modelo\n",
    "    rf.fit(X_train, y_train)\n",
    "#Predict\n",
    "    y_pred = rf.predict(X_test)\n",
    "#Metricas\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "#Factoring Importance\n",
    "    feature_importance = rf.feature_importances_\n",
    "    feature_names = X_test.columns\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return  r2, mse, mae, importance_df, rf\n",
    "\n",
    "# Neural Network\n",
    "\n",
    "def neural_network_model(X, y, test_size=0.2):\n",
    "    # Definir columnas por tipo\n",
    "    numeric_features = ['month', 'day', 'hour', 'ctx-1', 'ctx-2', 'ctx-3', 'ctx-4', 'lat', 'lon']\n",
    "    categorical_features = ['station_id', 'day_type']\n",
    "\n",
    "    # Preprocesado\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Modelo optimizado: 2 capas ocultas, early stopping, batch_size bajo\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        random_state=42,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        batch_size=256\n",
    "    )\n",
    "\n",
    "    # Pipeline de preprocesado + modelo\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "\n",
    "    # División de datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Entrenar\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predicción\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Métricas\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    return r2, mse, mae, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Métrica  Neural Network\n",
      "0      R²        0.795054\n",
      "1     MSE        0.015544\n",
      "2     MAE        0.085573\n",
      "Importancia de las características según Random Forest:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'importance_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics_df)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportancia de las características según Random Forest:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(importance_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'importance_df' is not defined"
     ]
    }
   ],
   "source": [
    "# r2_linear, mse_linear, mae_linear = linear_regression(X,y, 0.3)\n",
    "# r2_rf, mse_rf, mae_rf, importance_df,rf = RandomForest(X, y, 0.3)\n",
    "r2_nn, mse_nn, mae_nn, nn_model = neural_network_model(X, y, 0.3)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Métrica': ['R²', 'MSE', 'MAE'],\n",
    "    # 'Regresión Lineal': [r2_linear, mse_linear, mae_linear],\n",
    "    # 'Random Forest': [r2_rf, mse_rf, mae_rf],\n",
    "    'Neural Network': [r2_nn, mse_nn, mae_nn]\n",
    "})\n",
    "\n",
    "print(metrics_df)\n",
    "print(\"Importancia de las características según Random Forest:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo 'predictions.csv' creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "\n",
    "# --- CARGA PARCIAL DEL DATASET, INCLUYENDO 'index' ---\n",
    "use_cols = ['index', 'station_id', 'month', 'day', 'hour', 'ctx-4', 'ctx-3', 'ctx-2', 'ctx-1']\n",
    "df = pd.read_csv('../metadata_sample_submission_2025.csv', usecols=use_cols)\n",
    "\n",
    "df['year'] = 2024  # Necesario para `day_categorization_bcn`\n",
    "\n",
    "# --- IMPUTACIÓN DE LAT/LON ---\n",
    "df_merge_final = df_merge_final[['station_id', 'lat', 'lon']].drop_duplicates()\n",
    "df = df.merge(df_merge_final, on='station_id', how='left')\n",
    "\n",
    "# --- CLASIFICACIÓN DEL DÍA ---\n",
    "df = day_categorization_bcn(df)\n",
    "\n",
    "# --- CONVERSIÓN DE CATEGÓRICAS (si hace falta) ---\n",
    "if df['day_type'].dtype == 'object':\n",
    "    df['day_type'] = LabelEncoder().fit_transform(df['day_type'])\n",
    "\n",
    "# --- PREDICCIÓN POR LOTES ---\n",
    "features = ['station_id', 'month', 'day', 'hour', 'ctx-4', 'ctx-3', 'ctx-2', 'ctx-1', 'lat', 'lon', 'day_type']\n",
    "X_predict = df[features]\n",
    "\n",
    "batch_size = 5000\n",
    "predictions = []\n",
    "for start in range(0, len(X_predict), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch = X_predict.iloc[start:end]\n",
    "    preds = nn_model.predict(batch)\n",
    "    predictions.extend(preds)\n",
    "    del batch\n",
    "    gc.collect()\n",
    "\n",
    "# --- CREACIÓN DEL DF FINAL PARA ENTREGABLE ---\n",
    "df['percentage_docks_available'] = predictions\n",
    "df_final = df[['index', 'percentage_docks_available']]\n",
    "df_final.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"✅ Archivo 'predictions.csv' creado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['index'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_final \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercentage_docks_available\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      2\u001b[0m df_final\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['index'] not in index\""
     ]
    }
   ],
   "source": [
    "df_final = df[['index','percentage_docks_available']]\n",
    "df_final.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
