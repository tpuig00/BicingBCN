{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas dask scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BO\n",
    "import os  \n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "from dask.distributed import Client\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# --- FUNCIONES DE CARGA Y PREPROCESADO ---\n",
    "\n",
    "def inicialize_dask():\n",
    "    client = Client(memory_limit='8GB', processes=False)\n",
    "    print(client)\n",
    "\n",
    "def cargar_datos():\n",
    "    inicialize_dask()\n",
    "    data_path = '../data'\n",
    "    files = os.listdir(data_path)\n",
    "    \n",
    "    selected_files = []\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            parts = file.split('_')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            year = parts[0]\n",
    "            #if year not in ['2024', '2023', '2022']:\n",
    "            #   continue\n",
    "            if year not in ['2024']:\n",
    "                continue\n",
    "            month = parts[1]\n",
    "            #if year == '2024' and month not in ['05']:\n",
    "            #   continue\n",
    "            if year == '2024' and month not in ['06', '08', '10','12']:\n",
    "               continue\n",
    "            #if year == '2023' and month not in ['01', '11']:\n",
    "            #    continue\n",
    "            #if year == '2022' and month not in ['06']:\n",
    "            #    continue\n",
    "            selected_files.append(os.path.join(data_path, file))\n",
    "    \n",
    "    @delayed\n",
    "    def process_file(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, low_memory=True, dtype=str, \n",
    "             usecols=['station_id', 'last_reported', 'num_bikes_available',\n",
    "             'num_docks_available', 'num_bikes_available_types.mechanical', 'num_bikes_available_types.ebike', \n",
    "             'is_installed', 'is_renting', 'is_returning', 'is_charging_station'],\n",
    "            skiprows=lambda i: i > 0 and i % 3 != 0)\n",
    "            df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
    "            df['year'] = df['last_reported'].dt.year\n",
    "            df['month'] = df['last_reported'].dt.month\n",
    "            df['day'] = df['last_reported'].dt.day\n",
    "            df['hour'] = df['last_reported'].dt.hour\n",
    "\n",
    "            cols_to_drop = [col for col in ['traffic', 'V1'] if col in df.columns]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "            numeric_cols = ['num_bikes_available', 'num_docks_available', \n",
    "                            'num_bikes_available_types.mechanical', 'num_bikes_available_types.ebike']\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype(float)\n",
    "            \n",
    "            df = df.dropna(how='any')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    delayed_dfs = [process_file(file_path) for file_path in selected_files]\n",
    "    \n",
    "    if not delayed_dfs:\n",
    "        return None\n",
    "\n",
    "    ddf = dd.from_delayed(delayed_dfs)\n",
    "    df_meta = pd.read_csv('../Informacio_Estacions_Bicing_2025.csv',\n",
    "                          usecols=['station_id', 'lat', 'lon', 'capacity'],\n",
    "                          low_memory=False)\n",
    "    ddf['station_id'] = ddf['station_id'].astype('Int64')\n",
    "    df_meta['station_id'] = df_meta['station_id'].astype('Int64')\n",
    "\n",
    "    for col in ['is_installed', 'is_renting', 'is_returning']:\n",
    "        if col in ddf.columns:\n",
    "            ddf[col] = ddf[col].astype(str).replace({'nan': '0', '<NA>': '0'}).astype(int)\n",
    "    \n",
    "    if 'is_charging_station' in ddf.columns:\n",
    "        ddf['is_charging_station'] = ddf['is_charging_station'].astype(str).map({'TRUE': 1, 'FALSE': 0}).fillna(0).astype(int)\n",
    "    \n",
    "    ddf = ddf.dropna(how='any')\n",
    "    ddf = ddf.merge(df_meta, on='station_id', how='inner')\n",
    "    ddf['sum_capacity'] = ddf['num_bikes_available'] + ddf['num_docks_available']\n",
    "    df_final = ddf.compute()\n",
    "\n",
    "    median_capacity = df_final.groupby('station_id')['sum_capacity'].median()\n",
    "    df_final['capacity'] = df_final['capacity'].fillna(df_final['station_id'].map(median_capacity))\n",
    "    df_final['num_docks_available'] = df_final['num_docks_available'].clip(lower=0, upper=df_final['capacity'])\n",
    "    df_final['target'] = df_final['num_docks_available'] / df_final['capacity']\n",
    "    \n",
    "    aggregated_df = df_final.groupby(['station_id', 'year', 'month', 'day', 'hour']).agg(\n",
    "        num_bikes_available=('num_bikes_available', 'mean'),\n",
    "        num_docks_available=('num_docks_available', 'mean'),\n",
    "        num_mechanical=('num_bikes_available_types.mechanical', 'median'),\n",
    "        num_ebike=('num_bikes_available_types.ebike', 'median'),\n",
    "        is_renting=('is_renting', 'mean'),\n",
    "        is_returning=('is_returning', 'mean'),\n",
    "        target=('target', 'mean'),\n",
    "        lat=('lat', 'first'),\n",
    "        lon=('lon', 'first'),\n",
    "        capacity=('capacity', 'first')\n",
    "    ).reset_index()\n",
    "\n",
    "    id_df = pd.read_csv('../data/metadata_sample_submission_2025.csv')\n",
    "    station_list = pd.unique(id_df['station_id'])\n",
    "    aggregated_df = aggregated_df[aggregated_df['station_id'].isin(station_list)]\n",
    "    \n",
    "    return aggregated_df\n",
    "\n",
    "def crear_campos_optimized(df):\n",
    "    df = df.sort_values(by=['station_id', 'year', 'month', 'day', 'hour']).reset_index(drop=True)\n",
    "\n",
    "    for lag in range(1, 5):\n",
    "        df[f'ctx-{lag}'] = df.groupby('station_id')['target'].shift(lag)\n",
    "\n",
    "    mask = df.groupby('station_id').cumcount() >= 4\n",
    "    df = df[mask]\n",
    "    df = df.iloc[::5].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def day_categorization_bcn(df):\n",
    "    holiday_set = set([ \"2020-01-01\", \"2020-01-06\", \"2020-04-10\", \"2020-04-13\", \"2020-05-01\", \"2020-06-24\", \n",
    "                        \"2020-09-11\", \"2020-09-24\", \"2020-10-12\", \"2020-11-01\", \"2020-12-06\", \"2020-12-08\", \n",
    "                        \"2020-12-25\", \"2020-12-26\", \"2021-01-01\", \"2021-01-06\", \"2021-04-02\", \"2021-04-05\", \n",
    "                        \"2021-05-01\", \"2021-06-24\", \"2021-09-11\", \"2021-09-24\", \"2021-10-12\", \"2021-11-01\", \n",
    "                        \"2021-12-06\", \"2021-12-08\", \"2021-12-25\", \"2021-12-26\", \"2022-01-01\", \"2022-01-06\", \n",
    "                        \"2022-04-15\", \"2022-04-18\", \"2022-05-01\", \"2022-06-24\", \"2022-09-11\", \"2022-09-24\", \n",
    "                        \"2022-10-12\", \"2022-11-01\", \"2022-12-06\", \"2022-12-08\", \"2022-12-25\", \"2022-12-26\", \n",
    "                        \"2023-01-01\", \"2023-01-06\", \"2023-04-07\", \"2023-04-10\", \"2023-05-01\", \"2023-06-24\", \n",
    "                        \"2023-09-11\", \"2023-09-24\", \"2023-10-12\", \"2023-11-01\", \"2023-12-06\", \"2023-12-08\", \n",
    "                        \"2023-12-25\", \"2023-12-26\", \"2024-01-01\", \"2024-01-06\", \"2024-03-29\", \"2024-04-01\", \n",
    "                        \"2024-05-01\", \"2024-06-24\", \"2024-09-11\", \"2024-09-24\", \"2024-10-12\", \"2024-11-01\", \n",
    "                        \"2024-12-06\", \"2024-12-08\", \"2024-12-25\", \"2024-12-26\", \"2025-01-01\", \"2025-01-06\", \n",
    "                        \"2025-04-18\", \"2025-04-21\", \"2025-05-01\", \"2025-06-24\", \"2025-09-11\", \"2025-09-24\", \n",
    "                        \"2025-10-12\", \"2025-11-01\", \"2025-12-06\", \"2025-12-08\", \"2025-12-25\", \"2025-12-26\"])\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "    df['day_type'] = 0  # Default: workday\n",
    "    df.loc[df['date'].dt.weekday >= 5, 'day_type'] = 1  # Weekend\n",
    "    df.loc[df['date'].astype(str).isin(holiday_set), 'day_type'] = 2  # Holiday\n",
    "\n",
    "    return df.drop(columns=['date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U1054401\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 56316 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'inproc://192.168.1.37/5828/25' processes=1 threads=12, memory=7.45 GiB>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sort_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- PREPARACIÓN Y PREDICCIÓN ---\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Cargar y preparar los datos una vez\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df_merge \u001b[38;5;241m=\u001b[39m cargar_datos()\n\u001b[1;32m----> 5\u001b[0m df_merge \u001b[38;5;241m=\u001b[39m \u001b[43mcrear_campos_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_merge\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m df_merge_final \u001b[38;5;241m=\u001b[39m day_categorization_bcn(df_merge)\n\u001b[0;32m      8\u001b[0m X \u001b[38;5;241m=\u001b[39m df_merge_final[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctx-1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctx-2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctx-3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctx-4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_type\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "Cell \u001b[1;32mIn[8], line 128\u001b[0m, in \u001b[0;36mcrear_campos_optimized\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrear_campos_optimized\u001b[39m(df):\n\u001b[1;32m--> 128\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    131\u001b[0m         df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctx-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(lag)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sort_values'"
     ]
    }
   ],
   "source": [
    "# --- PREPARACIÓN Y PREDICCIÓN ---\n",
    "\n",
    "# Cargar y preparar los datos una vez\n",
    "df_merge = cargar_datos()\n",
    "df_merge = crear_campos_optimized(df_merge)\n",
    "df_merge_final = day_categorization_bcn(df_merge)\n",
    "\n",
    "X = df_merge_final[['station_id', 'month', 'day', 'hour', 'ctx-1', 'ctx-2', 'ctx-3', 'ctx-4', 'lat', 'lon', 'day_type']]\n",
    "y = df_merge_final['target']\n",
    "\n",
    "def neural_network_model(X, y, test_size=0.2):\n",
    "    # Filtrar datos de validación (junio a diciembre)\n",
    "    df_validation = X[(X['month'] >= 6) & (X['month'] <= 12)]\n",
    "    y_validation = y.loc[df_validation.index]\n",
    "    \n",
    "    # Separar los datos de entrenamiento (eliminando los de validación)\n",
    "    #df_train = X[~X.index.isin(df_validation.index)] no ho trec de moment\n",
    "    df_train = X[X.index.isin(df_validation.index)] \n",
    "    y_train = y.loc[df_train.index]\n",
    "\n",
    "    # Especificación de las columnas de entrada\n",
    "    percent_features = ['ctx-1', 'ctx-2', 'ctx-3', 'ctx-4']\n",
    "    bounded_features = ['month', 'day', 'hour']\n",
    "    continuous_features = ['lat', 'lon']\n",
    "    categorical_features = ['station_id', 'day_type']\n",
    "\n",
    "    # Asegurar que las columnas necesarias están en X_train\n",
    "    missing_cols = [col for col in percent_features + bounded_features + continuous_features + categorical_features if col not in df_train.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in X: {missing_cols}\")\n",
    "\n",
    "    # Definir las matrices X_train_data y X_validation_data como DataFrames\n",
    "    X_train_data = df_train[percent_features + bounded_features + continuous_features + categorical_features]\n",
    "    X_validation_data = df_validation[percent_features + bounded_features + continuous_features + categorical_features]\n",
    "\n",
    "    # Escalado de las etiquetas (target)\n",
    "    y_train_data = np.array(y_train).reshape(-1, 1)\n",
    "    y_validation_data = np.array(y_validation).reshape(-1, 1)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('percent', MinMaxScaler(), percent_features),\n",
    "                      ('bounded', MinMaxScaler(), bounded_features),\n",
    "                      ('continuous', StandardScaler(), continuous_features),\n",
    "                      ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)]\n",
    "    )\n",
    "\n",
    "    # Escalar las etiquetas (target)\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train_data)\n",
    "    y_validation_scaled = y_scaler.transform(y_validation_data)\n",
    "\n",
    "    # Definir el modelo\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        batch_size=128,\n",
    "        n_iter_no_change=10,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Crear el pipeline con preprocesamiento y modelo\n",
    "    pipeline = Pipeline([('preprocess', preprocessor), ('regressor', model)])\n",
    "    \n",
    "    # Ajustar el modelo\n",
    "    pipeline.fit(X_train_data, y_train_scaled.ravel())\n",
    "\n",
    "    # Realizar predicciones en el conjunto de validación\n",
    "    y_pred_scaled = pipeline.predict(X_validation_data)\n",
    "    y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    r2 = r2_score(y_validation_data, y_pred)\n",
    "    mse = mean_squared_error(y_validation_data, y_pred)\n",
    "    mae = mean_absolute_error(y_validation_data, y_pred)\n",
    "\n",
    "    return r2, mse, mae, pipeline, X_validation_data, y_validation_data\n",
    "\n",
    "# Resultados del modelo\n",
    "r2_nn, mse_nn, mae_nn, nn_model, _, _ = neural_network_model(X, y, 0.3)\n",
    "\n",
    "# Verificar valores antes de construir el DataFrame\n",
    "print(f\"R² NN: {r2_nn}, MSE NN: {mse_nn}, MAE NN: {mae_nn}\")\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Métrica': ['R²', 'MSE', 'MAE'],\n",
    "    'Neural Network': [r2_nn, mse_nn, mae_nn]\n",
    "})\n",
    "\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo 'predictions.csv' creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "\n",
    "# --- CARGA PARCIAL DEL DATASET, INCLUYENDO 'index' ---\n",
    "use_cols = ['index', 'station_id', 'month', 'day', 'hour', 'ctx-4', 'ctx-3', 'ctx-2', 'ctx-1']\n",
    "df = pd.read_csv('../data/metadata_sample_submission_2025.csv', usecols=use_cols)\n",
    "\n",
    "df['year'] = 2024  # Necesario para `day_categorization_bcn`\n",
    "\n",
    "# --- IMPUTACIÓN DE LAT/LON ---\n",
    "df_merge_final = df_merge_final[['station_id', 'lat', 'lon']].drop_duplicates()\n",
    "df = df.merge(df_merge_final, on='station_id', how='left')\n",
    "\n",
    "# --- CLASIFICACIÓN DEL DÍA ---\n",
    "df = day_categorization_bcn(df)\n",
    "\n",
    "# --- CONVERSIÓN DE CATEGÓRICAS (si hace falta) ---\n",
    "if df['day_type'].dtype == 'object':\n",
    "    df['day_type'] = LabelEncoder().fit_transform(df['day_type'])\n",
    "\n",
    "# --- PREDICCIÓN POR LOTES ---\n",
    "features = ['station_id', 'month', 'day', 'hour', 'ctx-4', 'ctx-3', 'ctx-2', 'ctx-1', 'lat', 'lon', 'day_type']\n",
    "X_predict = df[features]\n",
    "\n",
    "batch_size = 5000\n",
    "predictions = []\n",
    "\n",
    "for start in range(0, len(X_predict), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch = X_predict.iloc[start:end]\n",
    "    preds = nn_model.predict(batch)\n",
    "    predictions.extend(preds)\n",
    "    del batch\n",
    "    gc.collect()\n",
    "\n",
    "# --- CREACIÓN DEL DF FINAL PARA ENTREGABLE ---\n",
    "df['percentage_docks_available'] = predictions\n",
    "df_final = df[['index', 'percentage_docks_available']]\n",
    "df_final.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"✅ Archivo 'predictions.csv' creado correctamente.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
