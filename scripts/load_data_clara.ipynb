{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: dask in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (2025.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (2.0.1)\n",
      "Collecting streamlit\n",
      "  Obtaining dependency information for streamlit from https://files.pythonhosted.org/packages/46/9e/e51e34f504940da00145795b9e8be9c129704708b071f672f3626a37d842/streamlit-1.44.0-py3-none-any.whl.metadata\n",
      "  Downloading streamlit-1.44.0-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: seaborn in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from dask) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from dask) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from dask) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from dask) (24.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from dask) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from dask) (1.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Obtaining dependency information for altair<6,>=4.0 from https://files.pythonhosted.org/packages/aa/f3/0b6ced594e51cc95d8c1fc1640d3623770d01e4969d29c0bd09945fafefa/altair-5.5.0-py3-none-any.whl.metadata\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Obtaining dependency information for blinker<2,>=1.0.0 from https://files.pythonhosted.org/packages/10/cb/f2ad4230dc2eb1a74edf38f1a38b9b52277f75bef262d8908e60d957e13c/blinker-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Obtaining dependency information for cachetools<6,>=4.0 from https://files.pythonhosted.org/packages/72/76/20fa66124dbe6be5cafeb312ece67de6b61dd91a0247d1ea13db4ebb33c2/cachetools-5.5.2-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (11.0.0)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Obtaining dependency information for protobuf<6,>=3.20 from https://files.pythonhosted.org/packages/79/fc/2474b59570daa818de6124c0a15741ee3e5d6302e9d6ce0bdfd12e98119f/protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (2.32.3)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Obtaining dependency information for tenacity<10,>=8.1.0 from https://files.pythonhosted.org/packages/b6/cb/b86984bed139586d01532a587464b5805f12e397594f19f931c4c2fbfa61/tenacity-9.0.0-py3-none-any.whl.metadata\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Obtaining dependency information for toml<2,>=0.10.1 from https://files.pythonhosted.org/packages/44/6f/7120676b6d73228c96e17f1f794d8ab046fc910d781c8d151120c3f1569e/toml-0.10.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (4.12.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Obtaining dependency information for watchdog<7,>=2.1.5 from https://files.pythonhosted.org/packages/db/d9/c495884c6e548fce18a8f40568ff120bc3a4b7b99813081c8ac0c936fa64/watchdog-6.0.0-py3-none-win_amd64.whl.metadata\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.3/44.3 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Obtaining dependency information for gitpython!=3.1.19,<4,>=3.0.7 from https://files.pythonhosted.org/packages/1d/9a/4114a9057db2f1462d5c8f8390ab7383925fe1ac012eaa42402ad65c2963/GitPython-3.1.44-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Obtaining dependency information for pydeck<1,>=0.8.0b4 from https://files.pythonhosted.org/packages/ab/4c/b888e6cf58bd9db9c93f40d1c6be8283ff49d88919231afe93a6bcf61626/pydeck-0.9.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Obtaining dependency information for jsonschema>=3.0 from https://files.pythonhosted.org/packages/69/4a/4f9dbeb84e8850557c02365a0eee0649abe5eb1d84af92a25731c6c0f922/jsonschema-4.23.0-py3-none-any.whl.metadata\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Obtaining dependency information for narwhals>=1.14.2 from https://files.pythonhosted.org/packages/e9/96/79a6168dc7a5098066e097c01a45d01608c8df6552dfb92a2676ce623186/narwhals-1.32.0-py3-none-any.whl.metadata\n",
      "  Downloading narwhals-1.32.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.1->dask) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/a0/61/5c78b91c3143ed5c14207f463aecfc8f9dbb5092fb2869baf37c273b2705/gitdb-4.0.12-py3-none-any.whl.metadata\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: locket in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from partd>=1.4.0->dask) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/04/be/d09147ad1ec7934636ad912901c5fd7667e1c858e19d355237db0d0cd5e4/smmap-5.0.2-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\u1054401\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Obtaining dependency information for attrs>=22.2.0 from https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl.metadata\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/d1/0f/8910b19ac0670a0f80ce1008e5e751c4a57e14d2c4c13a482aa6079fa9d6/jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/c1/b1/3baf80dc6d2b7bc27a95a67752d0208e410351e3feb4eb78de5f77454d8d/referencing-0.36.2-py3-none-any.whl.metadata\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/bb/46/b8b5424d1d21f2f2f3f2d468660085318d4f74a8df8289e3dd6ad224d488/rpds_py-0.24.0-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading rpds_py-0.24.0-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Downloading streamlit-1.44.0-py3-none-any.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.7/9.8 MB 23.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.9/9.8 MB 24.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.4/9.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.7/9.8 MB 27.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.0/9.8 MB 27.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.8 MB 26.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.8 MB 27.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/9.8 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/9.8 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 23.2 MB/s eta 0:00:00\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 731.2/731.2 kB 15.3 MB/s eta 0:00:00\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "   ---------------------------------------- 0.0/207.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 207.6/207.6 kB 6.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "   ---------------------------------------- 0.0/434.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 434.5/434.5 kB 13.7 MB/s eta 0:00:00\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.6/6.9 MB 33.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.0/6.9 MB 31.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.5/6.9 MB 32.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.5/6.9 MB 31.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.9/6.9 MB 29.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.9/6.9 MB 29.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 23.2 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "   ---------------------------------------- 0.0/79.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 79.1/79.1 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.8/62.8 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "   ---------------------------------------- 0.0/88.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 88.5/88.5 kB 4.9 MB/s eta 0:00:00\n",
      "Downloading narwhals-1.32.0-py3-none-any.whl (320 kB)\n",
      "   ---------------------------------------- 0.0/320.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 320.1/320.1 kB 19.4 MB/s eta 0:00:00\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.8/63.8 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.24.0-cp312-cp312-win_amd64.whl (239 kB)\n",
      "   ---------------------------------------- 0.0/239.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 239.7/239.7 kB 7.4 MB/s eta 0:00:00\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: watchdog, toml, tenacity, smmap, rpds-py, protobuf, narwhals, cachetools, blinker, attrs, referencing, pydeck, gitdb, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed altair-5.5.0 attrs-25.3.0 blinker-1.9.0 cachetools-5.5.2 gitdb-4.0.12 gitpython-3.1.44 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 narwhals-1.32.0 protobuf-5.29.4 pydeck-0.9.1 referencing-0.36.2 rpds-py-0.24.0 smmap-5.0.2 streamlit-1.44.0 tenacity-9.0.0 toml-0.10.2 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install pandas dask scikit-learn numpy streamlit seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BO\n",
    "import os  \n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "from dask.distributed import Client\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# --- FUNCIONES DE CARGA Y PREPROCESADO ---\n",
    "\n",
    "def inicialize_dask():\n",
    "    client = Client(memory_limit='8GB', processes=False)\n",
    "    print(client)\n",
    "\n",
    "def cargar_datos():\n",
    "    inicialize_dask()\n",
    "    data_path = '../data'\n",
    "    files = os.listdir(data_path)\n",
    "    \n",
    "    selected_files = []\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            parts = file.split('_')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            year = parts[0]\n",
    "            #if year not in ['2024', '2023', '2022']:\n",
    "            #   continue\n",
    "            if year not in ['2024','2023']:\n",
    "                continue\n",
    "            month = parts[1]\n",
    "            #if year == '2024' and month not in ['05']:\n",
    "            #   continue\n",
    "            if year == '2020' and month not in ['06','07','08','09','10','11','12']:\n",
    "               continue\n",
    "            if year == '2021' and month not in ['06','07','08','09','10','11','12']:\n",
    "               continue\n",
    "            if year == '2022' and month not in ['06','07','08','09','10','11','12']:\n",
    "               continue\n",
    "            if year == '2023' and month not in ['06','07','08','09','10','11','12']:\n",
    "               continue\n",
    "            if year == '2024' and month not in ['06','07','08','09','10','11','12']:\n",
    "               continue\n",
    "            #if year == '2023' and month not in ['01', '11']:\n",
    "            #    continue\n",
    "            #if year == '2022' and month not in ['06']:\n",
    "            #    continue\n",
    "            selected_files.append(os.path.join(data_path, file))\n",
    "    \n",
    "    @delayed\n",
    "    def process_file(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, low_memory=True, dtype=str, \n",
    "             usecols=['station_id', 'last_reported', 'num_bikes_available',\n",
    "             'num_docks_available', 'num_bikes_available_types.mechanical', 'num_bikes_available_types.ebike', \n",
    "             'is_installed', 'is_renting', 'is_returning', 'is_charging_station'],\n",
    "            skiprows=lambda i: i > 0 and i % 3 != 0)\n",
    "            df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
    "            df['year'] = df['last_reported'].dt.year\n",
    "            df['month'] = df['last_reported'].dt.month\n",
    "            df['day'] = df['last_reported'].dt.day\n",
    "            df['hour'] = df['last_reported'].dt.hour\n",
    "\n",
    "            cols_to_drop = [col for col in ['traffic', 'V1'] if col in df.columns]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "            numeric_cols = ['num_bikes_available', 'num_docks_available', \n",
    "                            'num_bikes_available_types.mechanical', 'num_bikes_available_types.ebike']\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype(float)\n",
    "            \n",
    "            df = df.dropna(how='any')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    delayed_dfs = [process_file(file_path) for file_path in selected_files]\n",
    "    \n",
    "    if not delayed_dfs:\n",
    "        return None\n",
    "\n",
    "    ddf = dd.from_delayed(delayed_dfs)\n",
    "    df_meta = pd.read_csv('../Informacio_Estacions_Bicing_2025.csv',\n",
    "                          usecols=['station_id', 'lat', 'lon', 'capacity'],\n",
    "                          low_memory=False)\n",
    "    ddf['station_id'] = ddf['station_id'].astype('Int64')\n",
    "    df_meta['station_id'] = df_meta['station_id'].astype('Int64')\n",
    "\n",
    "    for col in ['is_installed', 'is_renting', 'is_returning']:\n",
    "        if col in ddf.columns:\n",
    "            ddf[col] = ddf[col].astype(str).replace({'nan': '0', '<NA>': '0'}).astype(int)\n",
    "    \n",
    "    if 'is_charging_station' in ddf.columns:\n",
    "        ddf['is_charging_station'] = ddf['is_charging_station'].astype(str).map({'TRUE': 1, 'FALSE': 0}).fillna(0).astype(int)\n",
    "    \n",
    "    ddf = ddf.dropna(how='any')\n",
    "    ddf = ddf.merge(df_meta, on='station_id', how='inner')\n",
    "    ddf['sum_capacity'] = ddf['num_bikes_available'] + ddf['num_docks_available']\n",
    "    df_final = ddf.compute()\n",
    "\n",
    "    median_capacity = df_final.groupby('station_id')['sum_capacity'].median()\n",
    "    df_final['capacity'] = df_final['capacity'].fillna(df_final['station_id'].map(median_capacity))\n",
    "    df_final['num_docks_available'] = df_final['num_docks_available'].clip(lower=0, upper=df_final['capacity'])\n",
    "    df_final['target'] = df_final['num_docks_available'] / df_final['capacity']\n",
    "    \n",
    "    aggregated_df = df_final.groupby(['station_id', 'year', 'month', 'day', 'hour']).agg(\n",
    "        num_bikes_available=('num_bikes_available', 'mean'),\n",
    "        num_docks_available=('num_docks_available', 'mean'),\n",
    "        num_mechanical=('num_bikes_available_types.mechanical', 'median'),\n",
    "        num_ebike=('num_bikes_available_types.ebike', 'median'),\n",
    "        is_renting=('is_renting', 'mean'),\n",
    "        is_returning=('is_returning', 'mean'),\n",
    "        target=('target', 'mean'),\n",
    "        lat=('lat', 'first'),\n",
    "        lon=('lon', 'first'),\n",
    "        capacity=('capacity', 'first')\n",
    "    ).reset_index()\n",
    "\n",
    "    id_df = pd.read_csv('../data/metadata_sample_submission_2025.csv')\n",
    "    station_list = pd.unique(id_df['station_id'])\n",
    "    aggregated_df = aggregated_df[aggregated_df['station_id'].isin(station_list)]\n",
    "    \n",
    "    return aggregated_df\n",
    "\n",
    "def crear_campos_optimized(df):\n",
    "    df = df.sort_values(by=['station_id', 'year', 'month', 'day', 'hour']).reset_index(drop=True)\n",
    "\n",
    "    for lag in range(1, 5):\n",
    "        df[f'ctx-{lag}'] = df.groupby('station_id')['target'].shift(lag)\n",
    "\n",
    "    mask = df.groupby('station_id').cumcount() >= 4\n",
    "    df = df[mask]\n",
    "    df = df.iloc[::5].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def day_categorization_bcn(df):\n",
    "    holiday_set = set([ \"2020-01-01\", \"2020-01-06\", \"2020-04-10\", \"2020-04-13\", \"2020-05-01\", \"2020-06-24\", \n",
    "                        \"2020-09-11\", \"2020-09-24\", \"2020-10-12\", \"2020-11-01\", \"2020-12-06\", \"2020-12-08\", \n",
    "                        \"2020-12-25\", \"2020-12-26\", \"2021-01-01\", \"2021-01-06\", \"2021-04-02\", \"2021-04-05\", \n",
    "                        \"2021-05-01\", \"2021-06-24\", \"2021-09-11\", \"2021-09-24\", \"2021-10-12\", \"2021-11-01\", \n",
    "                        \"2021-12-06\", \"2021-12-08\", \"2021-12-25\", \"2021-12-26\", \"2022-01-01\", \"2022-01-06\", \n",
    "                        \"2022-04-15\", \"2022-04-18\", \"2022-05-01\", \"2022-06-24\", \"2022-09-11\", \"2022-09-24\", \n",
    "                        \"2022-10-12\", \"2022-11-01\", \"2022-12-06\", \"2022-12-08\", \"2022-12-25\", \"2022-12-26\", \n",
    "                        \"2023-01-01\", \"2023-01-06\", \"2023-04-07\", \"2023-04-10\", \"2023-05-01\", \"2023-06-24\", \n",
    "                        \"2023-09-11\", \"2023-09-24\", \"2023-10-12\", \"2023-11-01\", \"2023-12-06\", \"2023-12-08\", \n",
    "                        \"2023-12-25\", \"2023-12-26\", \"2024-01-01\", \"2024-01-06\", \"2024-03-29\", \"2024-04-01\", \n",
    "                        \"2024-05-01\", \"2024-06-24\", \"2024-09-11\", \"2024-09-24\", \"2024-10-12\", \"2024-11-01\", \n",
    "                        \"2024-12-06\", \"2024-12-08\", \"2024-12-25\", \"2024-12-26\", \"2025-01-01\", \"2025-01-06\", \n",
    "                        \"2025-04-18\", \"2025-04-21\", \"2025-05-01\", \"2025-06-24\", \"2025-09-11\", \"2025-09-24\", \n",
    "                        \"2025-10-12\", \"2025-11-01\", \"2025-12-06\", \"2025-12-08\", \"2025-12-25\", \"2025-12-26\"])\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "    df['day_type'] = 0  # Default: workday\n",
    "    df.loc[df['date'].dt.weekday >= 5, 'day_type'] = 1  # Weekend\n",
    "    df.loc[df['date'].astype(str).isin(holiday_set), 'day_type'] = 2  # Holiday\n",
    "\n",
    "    return df.drop(columns=['date'])\n",
    "\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def weather_features(df_merge_final):\n",
    "    \"\"\"\n",
    "    Carga el archivo de clima 'export.csv' desde la ruta relativa, agrega tres variables \n",
    "    basadas en el clima y realiza un inner join con 'df_merge_final' por año, mes y día.\n",
    "    \n",
    "    - 'rainy_day': 1 si prcp > 1 mm, 0 si no.\n",
    "    - 'windy_day': 1 si wspd > 30 km/h, 0 si no.\n",
    "    - 'hot_day': 1 si tmax > 15°C, 0 si no.\n",
    "\n",
    "    Parámetros:\n",
    "    df_merge_final (pd.DataFrame): DataFrame con columnas 'year', 'month', 'day'.\n",
    "    \n",
    "    Retorna:\n",
    "    pd.DataFrame: DataFrame unido con las nuevas variables añadidas.\n",
    "    \"\"\"\n",
    "    # Cargar el archivo export.csv desde la ruta relativa\n",
    "    export = pd.read_csv('../export.csv', parse_dates=[\"date\"])\n",
    "    \n",
    "    # Crear las nuevas variables basadas en las condiciones meteorológicas\n",
    "    export[\"rainy_day\"] = export[\"prcp\"].apply(lambda x: 1 if x > 1 else 0)\n",
    "    export[\"windy_day\"] = export[\"wspd\"].apply(lambda x: 1 if x > 30 else 0)\n",
    "    export[\"hot_day\"] = export[\"tmax\"].apply(lambda x: 1 if x > 15 else 0)\n",
    "    \n",
    "    # Extraer año, mes y día de la columna 'date'\n",
    "    export[\"year\"] = export[\"date\"].dt.year\n",
    "    export[\"month\"] = export[\"date\"].dt.month\n",
    "    export[\"day\"] = export[\"date\"].dt.day\n",
    "    \n",
    "    # Hacer el inner join con df_merge_final\n",
    "    df_final = df_merge_final.merge(export, on=[\"year\", \"month\", \"day\"], how=\"inner\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# Crear las variables cíclicas\n",
    "def create_cyclic_features(df):\n",
    "    \"\"\"\n",
    "    Crea las columnas cíclicas para mes, día y hora.\n",
    "    \"\"\"\n",
    "    # Mes: Sine y Cosine\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # Día: Sine y Cosine\n",
    "    df['sin_day'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    \n",
    "    # Hora: Sine y Cosine\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U1054401\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51286 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'inproc://192.168.1.109/28324/19' processes=1 threads=12, memory=7.45 GiB>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:4210: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map function that you are using.\n",
      "  Before: .map(func)\n",
      "  After:  .map(func, meta=('is_charging_station', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta, method=\"map\"))\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "C:\\Users\\U1054401\\AppData\\Local\\Temp\\ipykernel_28324\\2564383148.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['last_reported'] = pd.to_datetime(df['last_reported'], unit='s', errors='coerce')\n",
      "2025-03-30 21:37:29,151 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.30 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:30,978 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 5.44 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:39,347 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.00 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:39,758 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 5.64 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:41,720 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 5.98 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:42,230 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 5.75 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:42,513 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 5.96 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:43,304 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 5.83 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:43,413 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 5.97 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:44,026 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 5.80 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:44,227 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.00 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:44,927 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 5.88 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:45,435 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 5.97 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:45,501 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 5.87 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:45,811 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 5.97 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:45,922 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 5.94 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:46,143 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 5.99 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:46,210 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 5.92 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:46,430 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 5.98 GiB -- Worker memory limit: 7.45 GiB\n",
      "2025-03-30 21:37:46,989 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 5.03 GiB -- Worker memory limit: 7.45 GiB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m df_merge_final \u001b[38;5;241m=\u001b[39m create_cyclic_features(df_merge_final)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Incluir las nuevas variables meteorológicas en las características\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m df_merge_final \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m(df_merge_final)\n\u001b[0;32m     14\u001b[0m X \u001b[38;5;241m=\u001b[39m df_merge_final[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctx-1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctx-2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctx-3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctx-4\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     15\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrainy_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindy_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhot_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     16\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msin_month\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcos_month\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msin_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcos_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msin_hour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcos_hour\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m df_merge_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- PREPARACIÓN Y PREDICCIÓN ---\n",
    "\n",
    "# Cargar y preparar los datos una vez\n",
    "df_merge = cargar_datos()\n",
    "df_merge = crear_campos_optimized(df_merge)\n",
    "df_merge_final = day_categorization_bcn(df_merge)\n",
    "df_merge_final = weather_features(df_merge_final)\n",
    "df_merge_final = create_cyclic_features(df_merge_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.00735257\n",
      "Validation score: 0.851734\n",
      "Iteration 2, loss = 0.00601316\n",
      "Validation score: 0.849345\n",
      "Iteration 3, loss = 0.00579944\n",
      "Validation score: 0.859588\n",
      "Iteration 4, loss = 0.00570752\n",
      "Validation score: 0.859250\n",
      "Iteration 5, loss = 0.00564644\n",
      "Validation score: 0.858508\n",
      "Iteration 6, loss = 0.00560631\n",
      "Validation score: 0.861069\n",
      "Iteration 7, loss = 0.00556869\n",
      "Validation score: 0.859532\n",
      "Iteration 8, loss = 0.00554986\n",
      "Validation score: 0.862301\n",
      "Iteration 9, loss = 0.00553932\n",
      "Validation score: 0.862466\n",
      "Iteration 10, loss = 0.00551942\n",
      "Validation score: 0.862599\n",
      "Iteration 11, loss = 0.00551047\n",
      "Validation score: 0.860671\n",
      "Iteration 12, loss = 0.00549889\n",
      "Validation score: 0.862791\n",
      "Iteration 13, loss = 0.00548419\n",
      "Validation score: 0.862853\n",
      "Iteration 14, loss = 0.00548266\n",
      "Validation score: 0.863081\n",
      "Iteration 15, loss = 0.00547130\n",
      "Validation score: 0.862562\n",
      "Iteration 16, loss = 0.00547349\n",
      "Validation score: 0.861995\n",
      "Iteration 17, loss = 0.00546199\n",
      "Validation score: 0.864383\n",
      "Iteration 18, loss = 0.00545514\n",
      "Validation score: 0.860575\n",
      "Iteration 19, loss = 0.00545390\n",
      "Validation score: 0.862851\n",
      "Iteration 20, loss = 0.00545380\n",
      "Validation score: 0.863605\n",
      "Iteration 21, loss = 0.00544710\n",
      "Validation score: 0.863212\n",
      "Iteration 22, loss = 0.00544572\n",
      "Validation score: 0.863995\n",
      "Iteration 23, loss = 0.00543992\n",
      "Validation score: 0.861591\n",
      "Iteration 24, loss = 0.00544135\n",
      "Validation score: 0.864029\n",
      "Iteration 25, loss = 0.00543864\n",
      "Validation score: 0.863923\n",
      "Iteration 26, loss = 0.00544153\n",
      "Validation score: 0.862189\n",
      "Iteration 27, loss = 0.00543682\n",
      "Validation score: 0.861625\n",
      "Iteration 28, loss = 0.00543686\n",
      "Validation score: 0.859601\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "R² NN: 0.8628958800368471, MSE NN: 0.01051929116796748, MAE NN: 0.06907279763334062\n",
      "  Métrica  Neural Network\n",
      "0      R²        0.862896\n",
      "1     MSE        0.010519\n",
      "2     MAE        0.069073\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Incluir las nuevas variables meteorológicas en las características\n",
    "\n",
    "X = df_merge_final[['station_id', 'month', 'day', 'hour', 'ctx-1', 'ctx-2', 'ctx-3', 'ctx-4', \n",
    "                    'lat', 'lon', 'day_type', 'rainy_day', 'windy_day', 'hot_day', \n",
    "                    'sin_month', 'cos_month', 'sin_day', 'cos_day', 'sin_hour', 'cos_hour']]\n",
    "y = df_merge_final['target']\n",
    "\n",
    "def neural_network_model(X, y, test_size=0.2):\n",
    "    # Dividir los datos en entrenamiento y validación\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Se divide en conjunto de entrenamiento y validación\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Especificación de las columnas de entrada\n",
    "    percent_features = ['ctx-1', 'ctx-2', 'ctx-3', 'ctx-4']\n",
    "    bounded_features = ['month', 'day', 'hour']\n",
    "    continuous_features = ['lat', 'lon']\n",
    "    categorical_features = ['station_id', 'day_type']\n",
    "    weather_features = ['rainy_day', 'windy_day', 'hot_day']  \n",
    "    cyclic_features = ['sin_month', 'cos_month', 'sin_day', 'cos_day', 'sin_hour', 'cos_hour']  # Variables cíclicas\n",
    "\n",
    "    # Aseguramos que las columnas necesarias estén en X_train\n",
    "    missing_cols = [col for col in percent_features + bounded_features + continuous_features + categorical_features + weather_features + cyclic_features if col not in X_train.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in X: {missing_cols}\")\n",
    "\n",
    "    # Preprocesamiento de las características\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('percent', MinMaxScaler(), percent_features),\n",
    "            ('bounded', MinMaxScaler(), bounded_features),\n",
    "            ('continuous', StandardScaler(), continuous_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "            ('weather', MinMaxScaler(), weather_features),\n",
    "            ('cyclic', StandardScaler(), cyclic_features)]  # Escalado de las variables cíclicas\n",
    "    )\n",
    "\n",
    "    # Escalado de las etiquetas (target)\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "    y_validation_scaled = y_scaler.transform(y_validation.values.reshape(-1, 1))\n",
    "\n",
    "    # Definir el modelo de Red Neuronal (MLPRegressor)\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        batch_size=128,\n",
    "        n_iter_no_change=10,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Crear pipeline con preprocesamiento y el modelo\n",
    "    pipeline = Pipeline([('preprocess', preprocessor), ('regressor', model)])\n",
    "    \n",
    "    # Ajustar el modelo\n",
    "    pipeline.fit(X_train, y_train_scaled.ravel())\n",
    "\n",
    "    # Realizar predicciones en el conjunto de validación\n",
    "    y_pred_scaled = pipeline.predict(X_validation)\n",
    "    y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    r2 = r2_score(y_validation_scaled, y_pred)\n",
    "    mse = mean_squared_error(y_validation_scaled, y_pred)\n",
    "    mae = mean_absolute_error(y_validation_scaled, y_pred)\n",
    "\n",
    "    return r2, mse, mae, pipeline, X_validation, y_validation\n",
    "\n",
    "# Resultados del modelo\n",
    "r2_nn, mse_nn, mae_nn, nn_model, X_validation_data, y_validation_data = neural_network_model(X, y, 0.3)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"R² NN: {r2_nn}, MSE NN: {mse_nn}, MAE NN: {mae_nn}\")\n",
    "\n",
    "# Crear el DataFrame con las métricas\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Métrica': ['R²', 'MSE', 'MAE'],\n",
    "    'Neural Network': [r2_nn, mse_nn, mae_nn]\n",
    "})\n",
    "\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo 'predictions.csv' creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "\n",
    "# --- CARGA PARCIAL DEL DATASET, INCLUYENDO 'index' ---\n",
    "use_cols = ['index', 'station_id', 'month', 'day', 'hour', 'ctx-4', 'ctx-3', 'ctx-2', 'ctx-1']\n",
    "df = pd.read_csv('../data/metadata_sample_submission_2025.csv', usecols=use_cols)\n",
    "\n",
    "df['year'] = 2024  # Necesario para `day_categorization_bcn`\n",
    "\n",
    "# --- IMPUTACIÓN DE LAT/LON ---\n",
    "df_merge_final = df_merge_final[['station_id', 'lat', 'lon']].drop_duplicates()\n",
    "df = df.merge(df_merge_final, on='station_id', how='left')\n",
    "\n",
    "# --- CLASIFICACIÓN DEL DÍA ---\n",
    "df = day_categorization_bcn(df)\n",
    "\n",
    "# --- AGREGAR VARIABLES METEOROLÓGICAS ---\n",
    "df = weather_features(df)\n",
    "\n",
    "# --- CONVERSIÓN DE CATEGÓRICAS (si hace falta) ---\n",
    "if df['day_type'].dtype == 'object':\n",
    "    df['day_type'] = LabelEncoder().fit_transform(df['day_type'])\n",
    "\n",
    "# --- CREACIÓN DE VARIABLES CÍCLICAS (ya definida en otro lugar) ---\n",
    "df = create_cyclic_features(df)\n",
    "\n",
    "# --- INCLUIR LAS NUEVAS VARIABLES METEOROLÓGICAS EN LAS CARACTERÍSTICAS DE ENTRADA ---\n",
    "features = ['station_id', 'month', 'day', 'hour', 'ctx-4', 'ctx-3', 'ctx-2', 'ctx-1', \n",
    "            'lat', 'lon', 'day_type', 'rainy_day', 'windy_day', 'hot_day', \n",
    "            'sin_month', 'cos_month', 'sin_day', 'cos_day', 'sin_hour', 'cos_hour']\n",
    "X_predict = df[features]\n",
    "\n",
    "# --- PREDICCIÓN POR LOTES ---\n",
    "batch_size = 5000\n",
    "predictions = []\n",
    "\n",
    "# Asumiendo que `nn_model` es el modelo entrenado y el pipeline ajustado\n",
    "# Vamos a hacer las predicciones usando el modelo ajustado para transformar las características de entrada y realizar la predicción\n",
    "\n",
    "for start in range(0, len(X_predict), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch = X_predict.iloc[start:end]\n",
    "    \n",
    "    # Realizar predicciones utilizando el modelo ajustado\n",
    "    preds = nn_model.predict(batch)\n",
    "   \n",
    "    # Almacenar las predicciones\n",
    "    predictions.extend(preds)\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del batch\n",
    "    gc.collect()\n",
    "\n",
    "# --- CREACIÓN DEL DF FINAL PARA ENTREGABLE ---\n",
    "df['percentage_docks_available'] = predictions\n",
    "df_final = df[['index', 'percentage_docks_available']]\n",
    "\n",
    "# Guardar el archivo final\n",
    "df_final.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"✅ Archivo 'predictions.csv' creado correctamente.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
